{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5f19235",
   "metadata": {},
   "source": [
    "# 전체 크롤링 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73dc5aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "올래길 7코스\n",
      "##### 0\n",
      "1 . 올레길 7코스(서귀포-월평 올레)\n",
      "Soso\n",
      "초반에 나오는 천지연폭포를 조망할수 있는 칠십리시공원으로부터 황우지선녀탕, 외돌개로 이어지는 길이 너무 멋지다...이후 바닷길을 오가며범섬을 계속 바라보며 풍경감상하며 지루함없이 길을 걸을수있다...법환포구에 카페, 식당들이 있어서 이곳에서 식사및 쉬어가면 좋을듯~ 켄싱턴리조트이후 강정항과 도착점까지의 후반부는 포장길이라 발의 피로도가 강해지는게 아쉽다...\n",
      "올레길 7코스는 제주에서도 최고의 관광지 답게 해안따라 각종 명소와 더불어 꽃길과 몽골몽골 자갈길등 감성이 충만한 코스입니다.다소 폐쇄되 우회한 길이 있어 아쉽지만 충분히 좋은 코스입니다.\n",
      "아름다운 올레길\n",
      "월평포구부터 거꾸로 해변따라 걷기시작했는데 전체적으로 아름다운 풍경 좋았습니다. 숲속 오솔길이 나오다가 점점 범섬, 섶섬, 문섬이 눈앞까지 보이면서 마지막 지점(7코스 초반부분) 선녀탕에서는 물속에 퐁당 다이빙도 했습니다. ^^ 올레센타본부에 방문한 후 근처 네거리식당에서 시원칼칼한 갈치국도 존맛이였어요!!!\n",
      "올레길 배스트3!\n",
      "재미 없다\n",
      "중간에 사유지 있어서 애 먹었네요\n",
      "어나더레벨의 경치를 보여줍니다 최고네요\n",
      "자연과 함께   어우러진 바다내음도 너무너무 상쾌하고 ~~~  ^^ 무엇보다도   대한민국의 파라다이스   같아요!!!~~^^@\n",
      "올레길 중간에 무서운 소가 있어서 가다가 되돌아왔어요 ㅠㅠ 공격당할 뻔 했어요\n",
      "힘들지만 노을이 하루의 피로를 말끔히 가져가는..\n",
      "한적한 바다해안가를 걸으며 힐링을 한거 같다.\n",
      "6시간 걸려 완주. 걷는 동안 마주친 풍경 하나하나가 예술일강정 돌길이 조금 힘들긴 했지만 올레길 최고의 코스!\n",
      "제주  올레길 과  오름. 숲 .  바다.  어디 한곳    아름답지 않은 곳이 없지만  7코스는  올레길 중 백미  라 할만 합니다.\n",
      "올레여행자센터에서 시작하여 멋진 바닷가를 걷는 코스. 올레길은 언제 걸어도 멋진곳~!\n",
      "제주올레길\n",
      "2 . 올레길 7-1코스(월드컵경기장-서귀포 올레)\n",
      "서귀포에서 멀리 한라산말고 가까이 우뚝솟아있는 고근산을 오르는 코스...우린 시내에서 출발하여 역방향으로 걸었는데 정방향보다는 좀 수월한듯 하다날이 비도오고 흐려서 한라산도 서귀포앞바다도 선명하게 보이지 않았지만 멋진 뷰가 있는곳이다...엉또폭포로 내려오는길에는 짧아도 꽤 깊고 어두운 숲길도 있다...엉또폭포이후 서귀포터미널까지의 길이 좀 지루하며 아파트단지 들어서면서 부터는 피로도가 많이 올라간다...\n",
      "제 생각엔 여기는 별 3개로 바뀌어야 할거같습니다..해발 400m산을 오르고 힘 다 빠지게 만들어놓고 내리막가다가 또 계단지옥,오르막지옥이에요...\n",
      "고근산 정상에 올랐을때 뷰가 끝내주더라구요. 오르막은 힘듭니다.\n",
      "초반에 시작부터 엉또폭포 지점 6km정도가 계속 오르막 입니다. 힘드니 참고 하세요~\n",
      "시작점부터 등산하듯 꾸준히 올라갑니다.  고근산부터는 한참 내리막입니다.\n",
      "no more reviews to load\n",
      "3 . 올레길 7코스 스탬프\n",
      "no review in extract\n",
      "no review in crawling\n",
      "no more reviews to load\n",
      "4 . 대구올레 폭포골가는길팔공산7코스\n",
      "no review in extract\n",
      "no review in crawling\n",
      "no more reviews to load\n",
      "not found\n",
      "올래길 7코스_reviews.csv saved.\n",
      "올래길 7코스_reviews.csv saved.\n",
      "올래길 7코스_reviews.csv saved.\n",
      "올래길 7코스_reviews.csv saved.\n",
      "올래길 7코스_reviews.csv saved.\n",
      "올래길 7코스_reviews.csv saved.\n",
      "올래길 7코스_reviews.csv saved.\n",
      "올래길 7코스_reviews.csv saved.\n",
      "올래길 7코스_reviews.csv saved.\n",
      "올래길 7코스_reviews.csv saved.\n",
      "올래길 7코스_reviews.csv saved.\n",
      "올래길 7코스_reviews.csv saved.\n",
      "올래길 7코스_reviews.csv saved.\n",
      "올래길 7코스_reviews.csv saved.\n",
      "올래길 7코스_reviews.csv saved.\n",
      "올래길 7코스_reviews.csv saved.\n",
      "올래길 7코스_reviews.csv saved.\n",
      "올래길 7코스_reviews.csv saved.\n",
      "올래길 7코스_reviews.csv saved.\n",
      "올래길 7코스_reviews.csv saved.\n",
      "올래길 7코스_reviews.csv saved.\n",
      "올래길 7코스_reviews.csv saved.\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from time import sleep\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "##############################################################  ############\n",
    "##################### variable related selenium ##########################\n",
    "##########################################################################\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "options.add_argument('lang=ko_KR')\n",
    "chromedriver_path = \"C:/chromedriver\"\n",
    "driver = webdriver.Chrome(os.path.join(os.getcwd(), chromedriver_path), options=options)  # chromedriver 열기\n",
    "\n",
    "reviews = []  # 중복 리뷰를 저장하지 않기 위한 리스트 생성\n",
    "\n",
    "def main():\n",
    "    global driver, load_wb, review_num\n",
    "\n",
    "    driver.implicitly_wait(4)  # 렌더링 될때까지 기다린다 4초\n",
    "    driver.get('https://map.kakao.com/')  # 주소 가져오기\n",
    "\n",
    "    # 검색할 목록\n",
    "    place_infos = [input(\"\")]\n",
    "\n",
    "    for i, place in enumerate(place_infos):\n",
    "        # delay\n",
    "        if i % 4 == 0 and i != 0:\n",
    "            sleep(5)\n",
    "        print(\"#####\", i)\n",
    "        search(place)\n",
    "        \n",
    "    # 저장된 리뷰를 CSV 파일에 저장\n",
    "    save_to_csv(place, reviews)\n",
    "        \n",
    "    driver.quit()\n",
    "    print(\"finish\")\n",
    "\n",
    "def search(place):\n",
    "    global driver\n",
    "\n",
    "    search_area = driver.find_element_by_xpath('//*[@id=\"search.keyword.query\"]')  # 검색 창\n",
    "    search_area.send_keys(place)  # 검색어 입력\n",
    "    driver.find_element_by_xpath('//*[@id=\"search.keyword.submit\"]').send_keys(Keys.ENTER)  # Enter로 검색\n",
    "    sleep(1)\n",
    "\n",
    "    # 검색된 정보가 있는 경우에만 탐색\n",
    "    # 1번 페이지 place list 읽기\n",
    "    html = driver.page_source\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    place_lists = soup.select('.placelist > .PlaceItem') # 검색된 장소 목록\n",
    "\n",
    "    # 검색된 첫 페이지 장소 목록 크롤링하기\n",
    "    crawling(place, place_lists)\n",
    "    search_area.clear()\n",
    "\n",
    "    # 우선 더보기 클릭해서 2페이지\n",
    "    try:\n",
    "        driver.find_element_by_xpath('//*[@id=\"info.search.place.more\"]').send_keys(Keys.ENTER)\n",
    "        sleep(1)\n",
    "\n",
    "        # 2~ 5페이지 읽기\n",
    "        for i in range(2, 6):\n",
    "            # 페이지 넘기기\n",
    "            xPath = '//*[@id=\"info.search.page.no' + str(i) + '\"]'\n",
    "            driver.find_element_by_xpath(xPath).send_keys(Keys.ENTER)\n",
    "            sleep(1)\n",
    "\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            place_lists = soup.select('.placelist > .PlaceItem') # 장소 목록 list\n",
    "\n",
    "            crawling(place, place_lists[:5])\n",
    "\n",
    "    except ElementNotInteractableException:\n",
    "        print('not found')\n",
    "    finally:\n",
    "        search_area.clear()\n",
    "\n",
    "def save_to_csv(place, reviews):\n",
    "    file_name = f\"{place}_reviews.csv\"\n",
    "    with open(file_name, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['place_name', 'review']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for place_name, review in reviews: \n",
    "            writer.writerow({'place_name': place_name, 'review': review})\n",
    "            print(f\"{file_name} saved.\")\n",
    "\n",
    "\n",
    "\n",
    "def crawling(place, place_lists):\n",
    "\n",
    "    while_flag = False\n",
    "    for i, place in enumerate(place_lists):\n",
    "        place_name = place.select('.head_item > .tit_name > .link_name')[0].text  # place name\n",
    "        place_address = place.select('.info_item > .addr > p')[0].text  # place address\n",
    "\n",
    "        detail_page_xpath = '//*[@id=\"info.search.place.list\"]/li[' + str(i + 1) + ']/div[5]/div[4]/a[1]'\n",
    "        driver.find_element_by_xpath(detail_page_xpath).send_keys(Keys.ENTER)\n",
    "        driver.switch_to.window(driver.window_handles[-1])  # 상세정보 탭으로 변환\n",
    "        sleep(1)\n",
    "\n",
    "        print(i+1,'.', place_name)\n",
    "\n",
    "        # 첫 페이지\n",
    "        extract_review(place_name)\n",
    "\n",
    "        # 2-5 페이지\n",
    "        idx = 3\n",
    "        try:\n",
    "            page_num = len(driver.find_elements_by_class_name('link_page')) # 페이지 수 찾기\n",
    "            for i in range(page_num-1):\n",
    "                # css selector를 이용해 리뷰 페이지 찾기\n",
    "                driver.find_element_by_css_selector('#mArticle > div.cont_evaluation > div.evaluation_review > div > a:nth-child(' + str(idx) +')').send_keys(Keys.ENTER)\n",
    "                sleep(1)\n",
    "                extract_review(place_name)\n",
    "                idx += 1\n",
    "            driver.find_element_by_link_text('후기 더보기').send_keys(Keys.ENTER) # 버튼 누르기\n",
    "            sleep(1)\n",
    "            extract_review(place_name) # 리뷰 추출\n",
    "        except (NoSuchElementException, ElementNotInteractableException):\n",
    "            print(\"no review in crawling\")\n",
    "\n",
    "        # 그 이후 페이지\n",
    "        crawl_next_reviews(place_name)\n",
    "\n",
    "        driver.close()\n",
    "        driver.switch_to.window(driver.window_handles[0])  # 검색 탭으로 전환\n",
    "\n",
    "def extract_review(place_name):\n",
    "    global driver, reviews\n",
    "    ret = True\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    review_lists = soup.select('.list_evaluation > li')\n",
    "\n",
    "    if len(review_lists) != 0:\n",
    "        for i, review in enumerate(review_lists):\n",
    "            comment = review.select('.txt_comment > span')\n",
    "            if comment:\n",
    "                comment_text = comment[0].text.strip()\n",
    "                if comment_text and (place_name, comment_text) not in reviews:  # 수정된 부분\n",
    "                    reviews.append((place_name, comment_text))  # 수정된 부분\n",
    "                    print(comment_text)\n",
    "    else:\n",
    "        print('no review in extract')\n",
    "        ret = False\n",
    "\n",
    "    return ret\n",
    "\n",
    "def crawl_next_reviews(place_name):\n",
    "    global driver\n",
    "    max_clicks = 5  # 최대 클릭 횟수 설정\n",
    "    click_count = 0 # 현재 클릭 횟수 초기화\n",
    "    while click_count < max_clicks:\n",
    "        try:\n",
    "            driver.find_element_by_link_text('후기 더보기').send_keys(Keys.ENTER)  # 후기 더보기 버튼 클릭\n",
    "            sleep(1)\n",
    "            if not extract_review(place_name):  # 리뷰 추출\n",
    "                break\n",
    "            click_count += 1  # 클릭 횟수 증가\n",
    "\n",
    "        except (NoSuchElementException, ElementNotInteractableException):\n",
    "            print(\"no more reviews to load\")\n",
    "            break\n",
    "\n",
    "            \n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30b7cfb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제주도 놀이공원\n",
      "1 . 제주돌문화공원\n",
      "2 . 스누피가든\n",
      "3 . 에코랜드테마파크\n",
      "4 . 9.81파크\n",
      "5 . 윈드1947카트테마파크\n",
      "1 . 다이나믹메이즈 제주도성읍점\n",
      "2 . 루나폴\n",
      "3 . 서프라이즈테마파크\n",
      "4 . 신화테마파크\n",
      "5 . 제주코코몽에코파크\n",
      "6 . 제주 유리의성 (휴관중)\n",
      "7 . 바운스슈퍼파크 제주센터\n",
      "8 . 액티브파크\n",
      "9 . 퍼시픽리솜 마린스테이지\n",
      "10 . 제주공룡랜드 (휴업중)\n",
      "11 . 테디베어하우스 테지움 제주점\n",
      "12 . 오늘은카트레이싱\n",
      "13 . 세리월드\n",
      "14 . 물의도시 베니스랜드\n",
      "15 . 금능석물원\n",
      "1 . 제주센트럴파크\n",
      "2 . 신화워터파크\n",
      "3 . 성읍랜드\n",
      "4 . 산방산랜드\n",
      "5 . 훈데르트바서파크\n",
      "6 . 노루생태관찰원\n",
      "7 . 무비랜드왁스뮤지엄 제주\n",
      "8 . 휘닉스제주 글라스하우스\n",
      "9 . 토이파크\n",
      "10 . 런닝맨 제주점\n",
      "11 . 고스트타운\n",
      "12 . 포레스트사파리\n",
      "13 . 중문미로파크\n",
      "14 . 제주워터월드\n",
      "15 . 제주거울미로 이상한나라의 앨리스\n",
      "1 . 히어로플레이파크 제주점\n",
      "2 . 가시리 조랑말체험장\n",
      "3 . 남원용암해수풀장\n",
      "4 . 세리월드 카트레이싱\n",
      "5 . 제주루지테마파크\n",
      "6 . 무병장수테마파크\n",
      "7 . 마린파크\n",
      "8 . 금호리조트 제주아쿠아나\n",
      "9 . 휘닉스제주섭지코지 바람의언덕\n",
      "10 . 펀테마파크\n",
      "11 . 제주포크테마파크\n",
      "12 . 다이노대발이파크 (휴업중)\n",
      "13 . 남제주 나누리파크\n",
      "14 . 고스트타운 고스트하우스\n",
      "15 . 휘닉스제주섭지코지 해안산책로\n",
      "1 . 제주러브랜드 도깨비광장\n",
      "2 . 렛츠런파크제주 아름다운승마장\n",
      "3 . 신화워터파크 실내워터파크동\n",
      "4 . 이상한나라의앨리스 서광카트장\n",
      "5 . 선녀와나무꾼 테마공원 추억의영화마을\n",
      "6 . 제주드론파크\n",
      "7 . 익스트림아일랜드\n",
      "8 . 아쿠아플라넷제주 오션데크\n",
      "9 . 렛츠런파크제주 매직포니\n",
      "10 . 헤이파크\n",
      "11 . 스위스스토어\n",
      "12 . 휘닉스제주 불턱BBQ\n",
      "13 . 제주돌문화공원 돌하르방\n",
      "14 . 롯데호텔제주 플레이토피아\n",
      "15 . 제주애견공원\n",
      "finish\n",
      "Data saved to top_25_places_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "options.add_argument('lang=ko_KR')\n",
    "chromedriver_path = \"C:/chromedriver\"\n",
    "driver = webdriver.Chrome(os.path.join(os.getcwd(), chromedriver_path), options=options)\n",
    "\n",
    "def search(place):\n",
    "    global driver\n",
    "\n",
    "    search_area = driver.find_element_by_xpath('//*[@id=\"search.keyword.query\"]')\n",
    "    search_area.send_keys(place)\n",
    "    driver.find_element_by_xpath('//*[@id=\"search.keyword.submit\"]').send_keys(Keys.ENTER)\n",
    "    sleep(1)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    place_lists = soup.select('.placelist > .PlaceItem')\n",
    "\n",
    "    crawling(place, place_lists[:5])\n",
    "    search_area.clear()\n",
    "\n",
    "    try:\n",
    "        driver.find_element_by_xpath('//*[@id=\"info.search.place.more\"]').send_keys(Keys.ENTER)\n",
    "        sleep(1)\n",
    "\n",
    "        for i in range(2, 6):\n",
    "            xPath = '//*[@id=\"info.search.page.no' + str(i) + '\"]'\n",
    "            driver.find_element_by_xpath(xPath).send_keys(Keys.ENTER)\n",
    "            sleep(1)\n",
    "\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            place_lists = soup.select('.placelist > .PlaceItem')\n",
    "\n",
    "            crawling(place, place_lists[:])\n",
    "\n",
    "    except ElementNotInteractableException:\n",
    "        print('not found')\n",
    "    finally:\n",
    "        search_area.clear()\n",
    "\n",
    "def crawling(place, place_lists):\n",
    "    global driver, all_data\n",
    "\n",
    "    for i, place in enumerate(place_lists):\n",
    "        place_name = place.select('.head_item > .tit_name > .link_name')[0].text\n",
    "\n",
    "        detail_page_xpath = '//*[@id=\"info.search.place.list\"]/li[' + str(i + 1) + ']/div[5]/div[4]/a[1]'\n",
    "        driver.find_element_by_xpath(detail_page_xpath).send_keys(Keys.ENTER)\n",
    "        driver.switch_to.window(driver.window_handles[-1])\n",
    "        sleep(1)\n",
    "\n",
    "        print(i+1, '.', place_name)        \n",
    "        \n",
    "        data_dict = extract_data()\n",
    "\n",
    "        if data_dict:\n",
    "            all_data.append(data_dict)\n",
    "\n",
    "        driver.close()\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "def find_element(xpath):\n",
    "    global driver\n",
    "\n",
    "    try:\n",
    "        element = driver.find_element_by_xpath(xpath)\n",
    "        return element.text\n",
    "    except NoSuchElementException:\n",
    "        return \"\"\n",
    "\n",
    "def extract_data():\n",
    "    global driver\n",
    "\n",
    "    place_name = find_element('//*[@id=\"mArticle\"]/div[1]/div[1]/div[2]/div/h2')\n",
    "    place_location = find_element('//*[@id=\"mArticle\"]/div[1]/div[2]/div[1]/div/span[1]')\n",
    "    place_feature = find_element('//*[@id=\"mArticle\"]/div[1]/div[1]/div[2]/div/div/span[1]')\n",
    "    place_star =  find_element('//*[@id=\"mArticle\"]/div[5]/div[1]/div/em')\n",
    "    operating_time = find_element('//*[@id=\"mArticle\"]/div[1]/div[2]/div[2]/div/div[1]/ul/li/span/span[1]')\n",
    "\n",
    "    if place_name:\n",
    "        data_dict = {\"관광지 이름\": place_name,\n",
    "            \"place_name\": place_name,\n",
    "            \"place_location\": place_location,\n",
    "            \"place_feature\": place_feature,\n",
    "            \"place_star\": place_star,\n",
    "            \"operating_time\": operating_time}\n",
    "        return data_dict\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    global driver, all_data\n",
    "    all_data = []\n",
    "    driver.implicitly_wait(1)\n",
    "    driver.get('https://map.kakao.com/')\n",
    "    \n",
    "    place_infos = [input(\"\")]\n",
    "    place_infos_str = \"\".join(place_infos)\n",
    "\n",
    "    for i, place in enumerate(place_infos):\n",
    "        if i % 4 == 0 and i != 0:\n",
    "            sleep(1)\n",
    "        search(place)\n",
    "\n",
    "    driver.quit()\n",
    "    print(\"finish\")\n",
    "\n",
    "    # Save all_data to a CSV file\n",
    "    if all_data:\n",
    "        data_frame = pd.DataFrame(all_data)\n",
    "        data_frame = data_frame[['관광지 이름', 'place_name', 'place_location', 'place_feature', 'place_star', 'operating_time']]\n",
    "        data_frame.to_csv(f\"{place_infos_str}_places_data.csv\", index=False, encoding='utf-8-sig')\n",
    "        print(\"Data saved to top_25_places_data.csv\")\n",
    "    else:\n",
    "        print(\"No data to save\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433634d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
